
integrations:
- integration_type: git_repo
  git_repo: mosaicml/llm-foundry
  git_branch: main
  pip_install: -e .[gpu]

command: |
  cd llm-foundry/scripts/
  composer eval/eval.py /mnt/config/parameters.yaml
image: mosaicml/pytorch:2.0.1_cu118-python3.10-ubuntu20.04
name: hf-8bit-eval

compute:
  gpus: 8

parameters:
  max_seq_len: 1024
  seed: 1
  precision: amp_fp16

  model_name_or_path: bigscience/bloom-1b7

  run_name: hf-8bit-eval
  seed: 1
  max_seq_len: 1024

  models:
  -
    model_name: ${model_name_or_path}
    model:
      name: hf_causal_lm
      pretrained_model_name_or_path: ${model_name_or_path}
      init_device: cpu
      pretrained: true
      load_in_8bit: true
    tokenizer:
      name: ${model_name_or_path}
      kwargs:
        model_max_length: ${max_seq_len}

  device_eval_batch_size: 4

  # With load_in_8bit, do not specify fsdp_config

  icl_tasks: 'eval/yamls/tasks_light.yaml'
